#acl acl_hierarchic: True
#acl was:read,write,admin jason:read,write,admin rbeezer:read,write,admin KiranSKedlaya:read,write,admin KiranKedlaya:read,write,admin ThomasJudson:read,write,admin DavidFarmer:read,write,admin

= Brain dump from Nathan Carter about writing CCLI grants =

Nathan Carter has a Type I (exploratory) [[http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0736644|grant]] for writing Lurch, a piece of software .  He's also been on a review panel for CCLI once and a panel leader once.  These are "brain dump" notes from him during our long conversation at Mathfest about how to write a CCLI proposal.  

  * good writing and organization is key.  What do they want to read first; what questions will they have from that, then answer those right THEN.  Lead the reader through a path of question-answer-question-answer

== Criteria for getting grant ==

Intellectual merit: is this a good idea?  Pitch your summary to your colleagues and see what they think.

Broader impact: NSF funds things that have benefit to lots of different types of people, etc.

=== Two other criteria ===
The next two things are *critical* to spend lots of time thinking and planning about.  Most people don't, and it makes a huge impact on whether the grant gets funded.

==== Dissemination ====

In relationship to broader impact, include a plan (3 pages or so) about your dissemination plan.  Specifically, where are you going to go, what are you talking about, why they'll accept you, so:

  * put website into math dl
  * PREP proposals
  * workshop proposals that may be included in the budget.
  * Get seven people that commit to using sage as we go along and are paid honorariums to write tutorials telling how to integrate Sage into their courses.

==== Evaluation ====
For exploratory proposals, it doesn't have to be a formal study of effectiveness.  But it still can't be just "we'll hand students surveys".  You have to be serious about getting impact information.

Two types of evaluation:
  * formative evaluation: halfway through, how am I doing and what should I do better.

  * summative evaluations: at the end, did I succeed?

It needs to be obvious that you care as much about the dissemination and evaluation as the NSF does.

== Some other random thoughts ==

Supplementary letters: get letters from something like 10 experts saying that they wish this should be funded.  Why it is valuable, why you should be leading it, etc.  You can shift some of your broader impact, etc. to these letters.  Since you only have a few pages to describe all the great things about your project, instead of using up all of your space, ask some of your reference letter writers to talk about some of the benefits in depth.

For Sage development: you have double impact, since you are bringing students in to develop WHILE you are doing the project, instead of waiting until the end.

To gauge how well your explanation is, pay attention to what happens when you explain the idea to others.  If there is something they are missing, make sure you explain it in the proposal.  This is the precursor to asking people for comments on your proposal.
